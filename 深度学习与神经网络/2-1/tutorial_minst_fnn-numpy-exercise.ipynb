{"cells":[{"cell_type":"markdown","metadata":{"id":"ksOC4CjLyREh"},"source":["## 准备数据"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"xRQK0pPZyREi","executionInfo":{"status":"ok","timestamp":1709965022994,"user_tz":-480,"elapsed":384,"user":{"displayName":"Ryan Ming","userId":"15132354585412009292"}}},"outputs":[],"source":["import os\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers, optimizers, datasets\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # or any {'0', '1', '2'}\n","\n","def mnist_dataset():\n","    (x, y), (x_test, y_test) = datasets.mnist.load_data()\n","    #normalize\n","    x = x/255.0\n","    x_test = x_test/255.0\n","\n","    return (x, y), (x_test, y_test)"]},{"cell_type":"markdown","metadata":{"id":"iiH8EVOfyREj"},"source":["## Demo numpy based auto differentiation"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"lpzjhvc7yREj","executionInfo":{"status":"ok","timestamp":1709965023381,"user_tz":-480,"elapsed":5,"user":{"displayName":"Ryan Ming","userId":"15132354585412009292"}}},"outputs":[],"source":["import numpy as np\n","\n","class Matmul:\n","    def __init__(self):\n","        self.mem = {}\n","\n","    def forward(self, x, W):\n","        h = np.matmul(x, W)\n","        self.mem={'x': x, 'W':W}\n","        return h\n","\n","    def backward(self, grad_y):\n","        '''\n","        x: shape(N, d)\n","        w: shape(d, d')\n","        grad_y: shape(N, d')\n","        '''\n","        x = self.mem['x']\n","        W = self.mem['W']\n","\n","        ####################\n","        '''计算矩阵乘法的对应的梯度'''\n","        grad_x = np.matmul(grad_y, W.T)\n","        grad_W = np.matmul(x.T, grad_y)\n","        ####################\n","        return grad_x, grad_W\n","\n","\n","class Relu:\n","    def __init__(self):\n","        self.mem = {}\n","\n","    def forward(self, x):\n","        self.mem['x']=x\n","        return np.where(x > 0, x, np.zeros_like(x))\n","\n","    def backward(self, grad_y):\n","        '''\n","        grad_y: same shape as x\n","        '''\n","        ####################\n","        '''计算relu 激活函数对应的梯度'''\n","        x = self.mem['x']\n","        grad_x = (x > 0).astype(np.float32) * grad_y\n","        ####################\n","        return grad_x\n","\n","\n","\n","class Softmax:\n","    '''\n","    softmax over last dimention\n","    '''\n","    def __init__(self):\n","        self.epsilon = 1e-12\n","        self.mem = {}\n","\n","    def forward(self, x):\n","        '''\n","        x: shape(N, c)\n","        '''\n","        x_exp = np.exp(x)\n","        partition = np.sum(x_exp, axis=1, keepdims=True)\n","        out = x_exp/(partition+self.epsilon)\n","\n","        self.mem['out'] = out\n","        self.mem['x_exp'] = x_exp\n","        return out\n","\n","    def backward(self, grad_y):\n","        '''\n","        grad_y: same shape as x\n","        '''\n","        s = self.mem['out']\n","        sisj = np.matmul(np.expand_dims(s,axis=2), np.expand_dims(s, axis=1)) # (N, c, c)\n","        g_y_exp = np.expand_dims(grad_y, axis=1)\n","        tmp = np.matmul(g_y_exp, sisj) #(N, 1, c)\n","        tmp = np.squeeze(tmp, axis=1)\n","        tmp = -tmp+grad_y*s\n","        return tmp\n","\n","class Log:\n","    '''\n","    softmax over last dimention\n","    '''\n","    def __init__(self):\n","        self.epsilon = 1e-12\n","        self.mem = {}\n","\n","    def forward(self, x):\n","        '''\n","        x: shape(N, c)\n","        '''\n","        out = np.log(x+self.epsilon)\n","\n","        self.mem['x'] = x\n","        return out\n","\n","    def backward(self, grad_y):\n","        '''\n","        grad_y: same shape as x\n","        '''\n","        x = self.mem['x']\n","\n","        return 1./(x+1e-12) * grad_y\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mUC0uSnTyREk"},"source":["## Gradient check"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QnWhbs55yREk","executionInfo":{"status":"ok","timestamp":1709965023381,"user_tz":-480,"elapsed":5,"user":{"displayName":"Ryan Ming","userId":"15132354585412009292"}},"outputId":"2db05613-5f39-40b3-a6d3-f0732be66efe"},"outputs":[{"output_type":"stream","name":"stdout","text":["(array([[-0.89401885, -2.11161629, -1.54825716,  0.21000535,  1.42898687,\n","        -1.4660146 ],\n","       [-0.89401885, -2.11161629, -1.54825716,  0.21000535,  1.42898687,\n","        -1.4660146 ],\n","       [-0.89401885, -2.11161629, -1.54825716,  0.21000535,  1.42898687,\n","        -1.4660146 ],\n","       [-0.89401885, -2.11161629, -1.54825716,  0.21000535,  1.42898687,\n","        -1.4660146 ],\n","       [-0.89401885, -2.11161629, -1.54825716,  0.21000535,  1.42898687,\n","        -1.4660146 ]]), array([[-0.80646335, -0.80646335, -0.80646335, -0.80646335],\n","       [ 1.65948367,  1.65948367,  1.65948367,  1.65948367],\n","       [-0.12871105, -0.12871105, -0.12871105, -0.12871105],\n","       [ 2.24526468,  2.24526468,  2.24526468,  2.24526468],\n","       [-1.82660994, -1.82660994, -1.82660994, -1.82660994],\n","       [ 0.68910413,  0.68910413,  0.68910413,  0.68910413]]))\n","tf.Tensor(\n","[[-0.89401885 -2.11161629 -1.54825716  0.21000535  1.42898687 -1.4660146 ]\n"," [-0.89401885 -2.11161629 -1.54825716  0.21000535  1.42898687 -1.4660146 ]\n"," [-0.89401885 -2.11161629 -1.54825716  0.21000535  1.42898687 -1.4660146 ]\n"," [-0.89401885 -2.11161629 -1.54825716  0.21000535  1.42898687 -1.4660146 ]\n"," [-0.89401885 -2.11161629 -1.54825716  0.21000535  1.42898687 -1.4660146 ]], shape=(5, 6), dtype=float64)\n","[[1. 0. 1. 0. 1. 1.]\n"," [1. 0. 1. 1. 0. 1.]\n"," [0. 1. 1. 1. 0. 1.]\n"," [1. 1. 0. 0. 0. 1.]\n"," [1. 1. 0. 1. 0. 1.]]\n","tf.Tensor(\n","[[1. 0. 1. 0. 1. 1.]\n"," [1. 0. 1. 1. 0. 1.]\n"," [0. 1. 1. 1. 0. 1.]\n"," [1. 1. 0. 0. 0. 1.]\n"," [1. 1. 0. 1. 0. 1.]], shape=(5, 6), dtype=float64)\n","[[0. 1. 0. 0. 0. 0.]\n"," [1. 1. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 0. 0. 1.]\n"," [1. 0. 0. 0. 0. 0.]]\n","[[-9.10494697e-10  4.82721261e-04 -9.59741362e-08 -5.10415400e-09\n","  -4.82464797e-04 -1.54474514e-07]\n"," [ 2.49943046e-01  7.89037645e-06 -3.41642776e-05 -4.29921197e-07\n","  -2.49916277e-01 -6.53060199e-08]\n"," [-4.45848859e-05 -5.05750378e-02 -2.62289771e-04  5.11145932e-02\n","  -1.99150455e-04 -3.35302703e-05]\n"," [-3.14829829e-03 -5.33109450e-06 -1.92379444e-04 -1.85310273e-03\n","  -3.76122996e-07  5.19948768e-03]\n"," [ 2.10912008e-05 -1.67568812e-08 -1.84202018e-05 -4.32135963e-07\n","  -7.93631606e-12 -2.22209824e-06]]\n","tf.Tensor(\n","[[-9.10494697e-10  4.82721261e-04 -9.59741362e-08 -5.10415400e-09\n","  -4.82464797e-04 -1.54474514e-07]\n"," [ 2.49943046e-01  7.89037645e-06 -3.41642776e-05 -4.29921197e-07\n","  -2.49916277e-01 -6.53060199e-08]\n"," [-4.45848859e-05 -5.05750378e-02 -2.62289771e-04  5.11145932e-02\n","  -1.99150455e-04 -3.35302703e-05]\n"," [-3.14829829e-03 -5.33109450e-06 -1.92379444e-04 -1.85310273e-03\n","  -3.76122996e-07  5.19948768e-03]\n"," [ 2.10912008e-05 -1.67568812e-08 -1.84202018e-05 -4.32135963e-07\n","  -7.93631606e-12 -2.22209824e-06]], shape=(5, 6), dtype=float64)\n","[[ 0.          1.53279843 -0.          0.          0.          0.        ]\n"," [-5.95656227  1.29966305 -0.         -0.         -0.         -0.        ]\n"," [-0.          0.         -0.          0.64361358  0.          0.        ]\n"," [ 0.         -0.         -0.          0.          0.         -0.93477925]\n"," [ 0.5139042   0.          0.          0.         -0.          0.        ]]\n","tf.Tensor(\n","[[ 0.          1.53279843 -0.          0.          0.          0.        ]\n"," [-5.95656227  1.29966305 -0.         -0.         -0.         -0.        ]\n"," [-0.          0.         -0.          0.64361358  0.          0.        ]\n"," [ 0.         -0.         -0.          0.          0.         -0.93477925]\n"," [ 0.5139042   0.          0.          0.         -0.          0.        ]], shape=(5, 6), dtype=float64)\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-18-03e02407d5c9>:94: RuntimeWarning: invalid value encountered in log\n","  out = np.log(x+self.epsilon)\n"]}],"source":["import tensorflow as tf\n","\n","x = np.random.normal(size=[5, 6])\n","W = np.random.normal(size=[6, 4])\n","aa = Matmul()\n","out = aa.forward(x, W) # shape(5, 4)\n","grad = aa.backward(np.ones_like(out))\n","print (grad)\n","\n","with tf.GradientTape() as tape:\n","    x, W = tf.constant(x), tf.constant(W)\n","    tape.watch(x)\n","    y = tf.matmul(x, W)\n","    loss = tf.reduce_sum(y)\n","    grads = tape.gradient(loss, x)\n","    print (grads)\n","\n","import tensorflow as tf\n","\n","x = np.random.normal(size=[5, 6])\n","aa = Relu()\n","out = aa.forward(x) # shape(5, 4)\n","grad = aa.backward(np.ones_like(out))\n","print (grad)\n","\n","with tf.GradientTape() as tape:\n","    x= tf.constant(x)\n","    tape.watch(x)\n","    y = tf.nn.relu(x)\n","    loss = tf.reduce_sum(y)\n","    grads = tape.gradient(loss, x)\n","    print (grads)\n","\n","import tensorflow as tf\n","x = np.random.normal(size=[5, 6], scale=5.0, loc=1)\n","label = np.zeros_like(x)\n","label[0, 1]=1.\n","label[1, 0]=1\n","label[1, 1]=1\n","label[2, 3]=1\n","label[3, 5]=1\n","label[4, 0]=1\n","print(label)\n","aa = Softmax()\n","out = aa.forward(x) # shape(5, 6)\n","grad = aa.backward(label)\n","print (grad)\n","\n","with tf.GradientTape() as tape:\n","    x= tf.constant(x)\n","    tape.watch(x)\n","    y = tf.nn.softmax(x)\n","    loss = tf.reduce_sum(y*label)\n","    grads = tape.gradient(loss, x)\n","    print (grads)\n","\n","import tensorflow as tf\n","\n","x = np.random.normal(size=[5, 6])\n","aa = Log()\n","out = aa.forward(x) # shape(5, 4)\n","grad = aa.backward(label)\n","print (grad)\n","\n","with tf.GradientTape() as tape:\n","    x= tf.constant(x)\n","    tape.watch(x)\n","    y = tf.math.log(x)\n","    loss = tf.reduce_sum(y*label)\n","    grads = tape.gradient(loss, x)\n","    print (grads)"]},{"cell_type":"markdown","metadata":{"id":"CUiPOrV9yREk"},"source":["# Final Gradient Check"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2l0HyAdJyREl","executionInfo":{"status":"ok","timestamp":1709965023381,"user_tz":-480,"elapsed":4,"user":{"displayName":"Ryan Ming","userId":"15132354585412009292"}},"outputId":"273d8840-2e5f-4906-ede5-3c00e098d566"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.00000000e+00 1.70476948e+00 0.00000000e+00 0.00000000e+00\n","  0.00000000e+00 0.00000000e+00]\n"," [3.30543090e+05 0.00000000e+00 0.00000000e+00 0.00000000e+00\n","  0.00000000e+00 0.00000000e+00]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 5.19887978e+00\n","  0.00000000e+00 0.00000000e+00]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n","  0.00000000e+00 3.41506452e+04]\n"," [1.37827242e+01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n","  0.00000000e+00 0.00000000e+00]]\n","----------------------------------------\n","[[0.00000000e+00 1.70476948e+00 0.00000000e+00 0.00000000e+00\n","  0.00000000e+00 0.00000000e+00]\n"," [3.30543199e+05 0.00000000e+00 0.00000000e+00 0.00000000e+00\n","  0.00000000e+00 0.00000000e+00]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 5.19887978e+00\n","  0.00000000e+00 0.00000000e+00]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n","  0.00000000e+00 3.41506464e+04]\n"," [1.37827242e+01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n","  0.00000000e+00 0.00000000e+00]]\n"]}],"source":["import tensorflow as tf\n","\n","label = np.zeros_like(x)\n","label[0, 1]=1.\n","label[1, 0]=1\n","label[2, 3]=1\n","label[3, 5]=1\n","label[4, 0]=1\n","\n","x = np.random.normal(size=[5, 6])\n","W1 = np.random.normal(size=[6, 5])\n","W2 = np.random.normal(size=[5, 6])\n","\n","mul_h1 = Matmul()\n","mul_h2 = Matmul()\n","relu = Relu()\n","softmax = Softmax()\n","log = Log()\n","\n","h1 = mul_h1.forward(x, W1) # shape(5, 4)\n","h1_relu = relu.forward(h1)\n","h2 = mul_h2.forward(h1_relu, W2)\n","h2_soft = softmax.forward(h2)\n","h2_log = log.forward(h2_soft)\n","\n","\n","h2_log_grad = log.backward(label)\n","h2_soft_grad = softmax.backward(h2_log_grad)\n","h2_grad, W2_grad = mul_h2.backward(h2_soft_grad)\n","h1_relu_grad = relu.backward(h2_grad)\n","h1_grad, W1_grad = mul_h1.backward(h1_relu_grad)\n","\n","print(h2_log_grad)\n","print('--'*20)\n","# print(W2_grad)\n","\n","with tf.GradientTape() as tape:\n","    x, W1, W2, label = tf.constant(x), tf.constant(W1), tf.constant(W2), tf.constant(label)\n","    tape.watch(W1)\n","    tape.watch(W2)\n","    h1 = tf.matmul(x, W1)\n","    h1_relu = tf.nn.relu(h1)\n","    h2 = tf.matmul(h1_relu, W2)\n","    prob = tf.nn.softmax(h2)\n","    log_prob = tf.math.log(prob)\n","    loss = tf.reduce_sum(label * log_prob)\n","    grads = tape.gradient(loss, [prob])\n","    print (grads[0].numpy())"]},{"cell_type":"markdown","metadata":{"id":"X5kjb1TlyREl"},"source":["## 建立模型"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"Ux_yI5k-yREl","executionInfo":{"status":"ok","timestamp":1709965023382,"user_tz":-480,"elapsed":3,"user":{"displayName":"Ryan Ming","userId":"15132354585412009292"}}},"outputs":[],"source":["class myModel:\n","    def __init__(self):\n","\n","        self.W1 = np.random.normal(size=[28*28+1, 100])\n","        self.W2 = np.random.normal(size=[100, 10])\n","\n","        self.mul_h1 = Matmul()\n","        self.mul_h2 = Matmul()\n","        self.relu = Relu()\n","        self.softmax = Softmax()\n","        self.log = Log()\n","\n","\n","    def forward(self, x):\n","        x = x.reshape(-1, 28*28)\n","        bias = np.ones(shape=[x.shape[0], 1])\n","        x = np.concatenate([x, bias], axis=1)\n","\n","        self.h1 = self.mul_h1.forward(x, self.W1) # shape(5, 4)\n","        self.h1_relu = self.relu.forward(self.h1)\n","        self.h2 = self.mul_h2.forward(self.h1_relu, self.W2)\n","        self.h2_soft = self.softmax.forward(self.h2)\n","        self.h2_log = self.log.forward(self.h2_soft)\n","\n","    def backward(self, label):\n","        self.h2_log_grad = self.log.backward(-label)\n","        self.h2_soft_grad = self.softmax.backward(self.h2_log_grad)\n","        self.h2_grad, self.W2_grad = self.mul_h2.backward(self.h2_soft_grad)\n","        self.h1_relu_grad = self.relu.backward(self.h2_grad)\n","        self.h1_grad, self.W1_grad = self.mul_h1.backward(self.h1_relu_grad)\n","\n","model = myModel()\n"]},{"cell_type":"markdown","metadata":{"id":"R-jaCtVtyREm"},"source":["## 计算 loss"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"tyefpRd8yREm","executionInfo":{"status":"ok","timestamp":1709965023382,"user_tz":-480,"elapsed":3,"user":{"displayName":"Ryan Ming","userId":"15132354585412009292"}}},"outputs":[],"source":["def compute_loss(log_prob, labels):\n","     return np.mean(np.sum(-log_prob*labels, axis=1))\n","\n","\n","def compute_accuracy(log_prob, labels):\n","    predictions = np.argmax(log_prob, axis=1)\n","    truth = np.argmax(labels, axis=1)\n","    return np.mean(predictions==truth)\n","\n","def train_one_step(model, x, y):\n","    model.forward(x)\n","    model.backward(y)\n","    model.W1 -= 1e-5* model.W1_grad\n","    model.W2 -= 1e-5* model.W2_grad\n","    loss = compute_loss(model.h2_log, y)\n","    accuracy = compute_accuracy(model.h2_log, y)\n","    return loss, accuracy\n","\n","def test(model, x, y):\n","    model.forward(x)\n","    loss = compute_loss(model.h2_log, y)\n","    accuracy = compute_accuracy(model.h2_log, y)\n","    return loss, accuracy"]},{"cell_type":"markdown","metadata":{"id":"yswcuBM0yREm"},"source":["## 实际训练"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WkQqx8VLyREm","executionInfo":{"status":"ok","timestamp":1709965108165,"user_tz":-480,"elapsed":83599,"user":{"displayName":"Ryan Ming","userId":"15132354585412009292"}},"outputId":"31f5aabf-809b-4873-f8ea-052d16c1f3e3"},"outputs":[{"output_type":"stream","name":"stdout","text":["epoch 0 : loss 23.872597227585526 ; accuracy 0.09471666666666667\n","epoch 1 : loss 21.596947651177352 ; accuracy 0.15365\n","epoch 2 : loss 19.87677017752967 ; accuracy 0.2263\n","epoch 3 : loss 18.521126464412486 ; accuracy 0.25511666666666666\n","epoch 4 : loss 17.12677242803379 ; accuracy 0.31876666666666664\n","epoch 5 : loss 15.66526176366565 ; accuracy 0.36788333333333334\n","epoch 6 : loss 14.565871791256333 ; accuracy 0.40353333333333335\n","epoch 7 : loss 13.636684302494244 ; accuracy 0.43775\n","epoch 8 : loss 12.915613339749816 ; accuracy 0.4642\n","epoch 9 : loss 12.314187887269648 ; accuracy 0.48875\n","epoch 10 : loss 11.834146713154404 ; accuracy 0.50325\n","epoch 11 : loss 11.352046531245241 ; accuracy 0.5234833333333333\n","epoch 12 : loss 10.981684576566048 ; accuracy 0.5332\n","epoch 13 : loss 10.493938182974173 ; accuracy 0.5486333333333333\n","epoch 14 : loss 10.192585951046992 ; accuracy 0.5600666666666667\n","epoch 15 : loss 9.349641064528187 ; accuracy 0.5886666666666667\n","epoch 16 : loss 9.036968929334172 ; accuracy 0.6040833333333333\n","epoch 17 : loss 8.430857665972255 ; accuracy 0.6207333333333334\n","epoch 18 : loss 8.150795627681322 ; accuracy 0.63615\n","epoch 19 : loss 7.592817307124467 ; accuracy 0.6524666666666666\n","epoch 20 : loss 7.347038670438147 ; accuracy 0.6657333333333333\n","epoch 21 : loss 6.955661525845415 ; accuracy 0.6755333333333333\n","epoch 22 : loss 6.751585509793175 ; accuracy 0.6889333333333333\n","epoch 23 : loss 6.5689761089432315 ; accuracy 0.6917833333333333\n","epoch 24 : loss 6.521072546928833 ; accuracy 0.7019833333333333\n","epoch 25 : loss 6.094130692492536 ; accuracy 0.7131\n","epoch 26 : loss 5.979633524006001 ; accuracy 0.7218833333333333\n","epoch 27 : loss 5.796608042674626 ; accuracy 0.72445\n","epoch 28 : loss 5.986227767391529 ; accuracy 0.72385\n","epoch 29 : loss 5.204745094997366 ; accuracy 0.7512833333333333\n","epoch 30 : loss 5.038092136815563 ; accuracy 0.7566833333333334\n","epoch 31 : loss 5.056573647303745 ; accuracy 0.7551833333333333\n","epoch 32 : loss 5.297047794495397 ; accuracy 0.7502666666666666\n","epoch 33 : loss 4.794531072402079 ; accuracy 0.7674666666666666\n","epoch 34 : loss 4.85319605513935 ; accuracy 0.7656833333333334\n","epoch 35 : loss 4.751221966001747 ; accuracy 0.7688666666666667\n","epoch 36 : loss 4.809684554553628 ; accuracy 0.7683\n","epoch 37 : loss 4.523169518964207 ; accuracy 0.7794166666666666\n","epoch 38 : loss 4.468121048003598 ; accuracy 0.7820333333333334\n","epoch 39 : loss 4.338060451596959 ; accuracy 0.7870333333333334\n","epoch 40 : loss 4.277504147331552 ; accuracy 0.7905833333333333\n","epoch 41 : loss 4.127085145306959 ; accuracy 0.7968\n","epoch 42 : loss 4.050197442474066 ; accuracy 0.8008666666666666\n","epoch 43 : loss 3.9133254384739935 ; accuracy 0.8063333333333333\n","epoch 44 : loss 3.86091822841437 ; accuracy 0.80835\n","epoch 45 : loss 3.753699863465903 ; accuracy 0.81275\n","epoch 46 : loss 3.7205682554670263 ; accuracy 0.8149166666666666\n","epoch 47 : loss 3.6386886070704967 ; accuracy 0.8176833333333333\n","epoch 48 : loss 3.612879706992455 ; accuracy 0.8194666666666667\n","epoch 49 : loss 3.5489914225657557 ; accuracy 0.8215666666666667\n","test loss 3.3631837931482145 ; accuracy 0.8305\n"]}],"source":["train_data, test_data = mnist_dataset()\n","train_label = np.zeros(shape=[train_data[0].shape[0], 10])\n","test_label = np.zeros(shape=[test_data[0].shape[0], 10])\n","train_label[np.arange(train_data[0].shape[0]), np.array(train_data[1])] = 1.\n","test_label[np.arange(test_data[0].shape[0]), np.array(test_data[1])] = 1.\n","\n","for epoch in range(50):\n","    loss, accuracy = train_one_step(model, train_data[0], train_label)\n","    print('epoch', epoch, ': loss', loss, '; accuracy', accuracy)\n","loss, accuracy = test(model, test_data[0], test_label)\n","\n","print('test loss', loss, '; accuracy', accuracy)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加法进位实验\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/JerrikEph/jerrikeph.github.io/raw/master/Learn2Carry.png\" width=650>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-25 19:42:44.234702: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import layers, optimizers, datasets\n",
    "import os,sys,tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据生成\n",
    "我们随机在 `start->end`之间采样除整数对`(num1, num2)`，计算结果`num1+num2`作为监督信号。\n",
    "\n",
    "* 首先将数字转换成数字位列表 `convertNum2Digits`\n",
    "* 将数字位列表反向\n",
    "* 将数字位列表填充到同样的长度 `pad2len`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data_batch(batch_size, start, end):\n",
    "    '''在(start, end)区间采样生成一个batch的整型的数据\n",
    "    Args :\n",
    "        batch_size: batch_size\n",
    "        start: 开始数值\n",
    "        end: 结束数值\n",
    "    '''\n",
    "    numbers_1 = np.random.randint(start, end, batch_size)\n",
    "    numbers_2 = np.random.randint(start, end, batch_size)\n",
    "    results = numbers_1 + numbers_2\n",
    "    return numbers_1, numbers_2, results\n",
    "\n",
    "def convertNum2Digits(Num):\n",
    "    '''将一个整数转换成一个数字位的列表,例如 133412 ==> [1, 3, 3, 4, 1, 2]\n",
    "    '''\n",
    "    strNum = str(Num)\n",
    "    chNums = list(strNum)\n",
    "    digitNums = [int(o) for o in strNum]\n",
    "    return digitNums\n",
    "\n",
    "def convertDigits2Num(Digits):\n",
    "    '''将数字位列表反向， 例如 [1, 3, 3, 4, 1, 2] ==> [2, 1, 4, 3, 3, 1]\n",
    "    '''\n",
    "    digitStrs = [str(o) for o in Digits]\n",
    "    numStr = ''.join(digitStrs)\n",
    "    Num = int(numStr)\n",
    "    return Num\n",
    "\n",
    "def pad2len(lst, length, pad=0):\n",
    "    '''将一个列表用`pad`填充到`length`的长度 例如 pad2len([1, 3, 2, 3], 6, pad=0) ==> [1, 3, 2, 3, 0, 0]\n",
    "    '''\n",
    "    lst+=[pad]*(length - len(lst))\n",
    "    return lst\n",
    "\n",
    "def results_converter(res_lst):\n",
    "    '''将预测好的数字位列表批量转换成为原始整数\n",
    "    Args:\n",
    "        res_lst: shape(b_sz, len(digits))\n",
    "    '''\n",
    "    res = [reversed(digits) for digits in res_lst]\n",
    "    return [convertDigits2Num(digits) for digits in res]\n",
    "\n",
    "def prepare_batch(Nums1, Nums2, results, maxlen):\n",
    "    '''准备一个batch的数据，将数值转换成反转的数位列表并且填充到固定长度\n",
    "    Args:\n",
    "        Nums1: shape(batch_size,)\n",
    "        Nums2: shape(batch_size,)\n",
    "        results: shape(batch_size,)\n",
    "        maxlen:  type(int)\n",
    "    Returns:\n",
    "        Nums1: shape(batch_size, maxlen)\n",
    "        Nums2: shape(batch_size, maxlen)\n",
    "        results: shape(batch_size, maxlen)\n",
    "    '''\n",
    "    Nums1 = [convertNum2Digits(o) for o in Nums1]\n",
    "    Nums2 = [convertNum2Digits(o) for o in Nums2]\n",
    "    results = [convertNum2Digits(o) for o in results]\n",
    "    \n",
    "    Nums1 = [list(reversed(o)) for o in Nums1]\n",
    "    Nums2 = [list(reversed(o)) for o in Nums2]\n",
    "    results = [list(reversed(o)) for o in results]\n",
    "    \n",
    "    Nums1 = [pad2len(o, maxlen) for o in Nums1]\n",
    "    Nums2 = [pad2len(o, maxlen) for o in Nums2]\n",
    "    results = [pad2len(o, maxlen) for o in results]\n",
    "    \n",
    "    return Nums1, Nums2, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 建模过程， 按照图示完成建模"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/JerrikEph/jerrikeph.github.io/raw/master/Learn2Carry.png\" width=650>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myRNNModel(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(myRNNModel, self).__init__()\n",
    "        self.embed_layer = tf.keras.layers.Embedding(10, 32, \n",
    "                                                    batch_input_shape=[None, None])\n",
    "        \n",
    "        self.rnncell = tf.keras.layers.SimpleRNNCell(64)\n",
    "        self.rnn_layer = tf.keras.layers.RNN(self.rnncell, return_sequences=True)\n",
    "        self.dense = tf.keras.layers.Dense(10)\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, num1, num2):\n",
    "        '''\n",
    "        此处完成上述图中模型\n",
    "        '''\n",
    "        embedded_num1 = self.embed_layer(num1)\n",
    "        embedded_num2 = self.embed_layer(num2)\n",
    "        rnn_input = tf.concat([embedded_num1, embedded_num2], axis=-1)\n",
    "        rnn_output = self.rnn_layer(rnn_input)\n",
    "        logits = self.dense(rnn_output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_loss(logits, labels):\n",
    "    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=labels)\n",
    "    return tf.reduce_mean(losses)\n",
    "\n",
    "@tf.function\n",
    "def train_one_step(model, optimizer, x, y, label):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x, y)\n",
    "        loss = compute_loss(logits, label)\n",
    "\n",
    "    # compute gradient\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "def train(steps, model, optimizer):\n",
    "    loss = 0.0\n",
    "    accuracy = 0.0\n",
    "    for step in range(steps):\n",
    "        datas = gen_data_batch(batch_size=200, start=0, end=555555555)\n",
    "        Nums1, Nums2, results = prepare_batch(*datas, maxlen=11)\n",
    "        loss = train_one_step(model, optimizer, tf.constant(Nums1, dtype=tf.int32), \n",
    "                              tf.constant(Nums2, dtype=tf.int32),\n",
    "                              tf.constant(results, dtype=tf.int32))\n",
    "        if step%50 == 0:\n",
    "            print('step', step, ': loss',loss.numpy())\n",
    "\n",
    "    return loss\n",
    "\n",
    "def evaluate(model):\n",
    "    datas = gen_data_batch(batch_size=2000, start=555555555, end=999999999)\n",
    "    Nums1, Nums2, results = prepare_batch(*datas, maxlen=11)\n",
    "    logits = model(tf.constant(Nums1, dtype=tf.int32), tf.constant(Nums2, dtype=tf.int32))\n",
    "    logits = logits.numpy()\n",
    "    pred = np.argmax(logits, axis=-1)\n",
    "    res = results_converter(pred)\n",
    "    for o in list(zip(datas[2], res))[:20]:\n",
    "        print(o[0], o[1], o[0]==o[1])\n",
    "\n",
    "    print('accuracy is: %g' % np.mean([o[0]==o[1] for o in zip(datas[2], res)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-25 19:42:45.336882: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2024-03-25 19:42:45.382219: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:c2:00.0 name: NVIDIA GeForce RTX 4090 D computeCapability: 8.9\n",
      "coreClock: 2.52GHz coreCount: 114 deviceMemorySize: 23.64GiB deviceMemoryBandwidth: 938.86GiB/s\n",
      "2024-03-25 19:42:45.382249: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2024-03-25 19:42:45.385299: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2024-03-25 19:42:45.385365: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2024-03-25 19:42:45.386366: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2024-03-25 19:42:45.386611: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2024-03-25 19:42:45.387356: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2024-03-25 19:42:45.387939: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2024-03-25 19:42:45.388081: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2024-03-25 19:42:45.388989: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2024-03-25 19:42:45.389306: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-25 19:42:45.402339: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:c2:00.0 name: NVIDIA GeForce RTX 4090 D computeCapability: 8.9\n",
      "coreClock: 2.52GHz coreCount: 114 deviceMemorySize: 23.64GiB deviceMemoryBandwidth: 938.86GiB/s\n",
      "2024-03-25 19:42:45.403413: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2024-03-25 19:42:45.403478: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2024-03-25 19:42:45.912841: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2024-03-25 19:42:45.912864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
      "2024-03-25 19:42:45.912869: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
      "2024-03-25 19:42:45.914235: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22171 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 4090 D, pci bus id: 0000:c2:00.0, compute capability: 8.9)\n"
     ]
    }
   ],
   "source": [
    "optimizer = optimizers.Adam(0.001)\n",
    "model = myRNNModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-25 19:42:46.853805: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2024-03-25 19:42:46.888327: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2250125000 Hz\n",
      "2024-03-25 19:42:47.016703: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2024-03-25 19:42:47.568718: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2024-03-25 19:42:47.568762: I tensorflow/stream_executor/cuda/cuda_blas.cc:1838] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 : loss 2.306542\n",
      "step 50 : loss 1.9303919\n",
      "step 100 : loss 1.9024751\n",
      "step 150 : loss 1.8993219\n",
      "step 200 : loss 1.8866391\n",
      "step 250 : loss 1.8898065\n",
      "step 300 : loss 1.8997463\n",
      "step 350 : loss 1.8859814\n",
      "step 400 : loss 1.8806479\n",
      "step 450 : loss 1.8819673\n",
      "step 500 : loss 1.8783911\n",
      "step 550 : loss 1.8728806\n",
      "step 600 : loss 1.8773971\n",
      "step 650 : loss 1.8811697\n",
      "step 700 : loss 1.8866857\n",
      "step 750 : loss 1.8713734\n",
      "step 800 : loss 1.8729467\n",
      "step 850 : loss 1.8711131\n",
      "step 900 : loss 1.8731401\n",
      "step 950 : loss 1.8767232\n",
      "step 1000 : loss 1.8620554\n",
      "step 1050 : loss 1.887664\n",
      "step 1100 : loss 1.8600777\n",
      "step 1150 : loss 1.8629845\n",
      "step 1200 : loss 1.8526545\n",
      "step 1250 : loss 1.8510766\n",
      "step 1300 : loss 1.837868\n",
      "step 1350 : loss 1.7898852\n",
      "step 1400 : loss 1.7049842\n",
      "step 1450 : loss 1.6162711\n",
      "step 1500 : loss 1.4763733\n",
      "step 1550 : loss 1.311079\n",
      "step 1600 : loss 1.170388\n",
      "step 1650 : loss 1.0356289\n",
      "step 1700 : loss 0.9204894\n",
      "step 1750 : loss 0.83314866\n",
      "step 1800 : loss 0.75900656\n",
      "step 1850 : loss 0.68052757\n",
      "step 1900 : loss 0.6066742\n",
      "step 1950 : loss 0.5371122\n",
      "step 2000 : loss 0.4655651\n",
      "step 2050 : loss 0.40266693\n",
      "step 2100 : loss 0.3427712\n",
      "step 2150 : loss 0.2889852\n",
      "step 2200 : loss 0.24667661\n",
      "step 2250 : loss 0.21427062\n",
      "step 2300 : loss 0.17972654\n",
      "step 2350 : loss 0.15205884\n",
      "step 2400 : loss 0.12975903\n",
      "step 2450 : loss 0.11752968\n",
      "step 2500 : loss 0.10002859\n",
      "step 2550 : loss 0.08838905\n",
      "step 2600 : loss 0.074607044\n",
      "step 2650 : loss 0.06579953\n",
      "step 2700 : loss 0.059068896\n",
      "step 2750 : loss 0.051986493\n",
      "step 2800 : loss 0.046696503\n",
      "step 2850 : loss 0.041333232\n",
      "step 2900 : loss 0.037638955\n",
      "step 2950 : loss 0.03395289\n",
      "1683519251 1683519251 True\n",
      "1528632292 1528632292 True\n",
      "1672584209 1672584209 True\n",
      "1434750756 1434750756 True\n",
      "1568000088 1568000088 True\n",
      "1924080177 1924080177 True\n",
      "1676906784 1676906784 True\n",
      "1636019092 1636019092 True\n",
      "1234203357 1234203357 True\n",
      "1570970528 1570970528 True\n",
      "1318045624 1318045624 True\n",
      "1528343610 1528343610 True\n",
      "1749761868 1749761868 True\n",
      "1894918324 1894918324 True\n",
      "1540570948 1540570948 True\n",
      "1650946307 1650946307 True\n",
      "1694603158 1694603158 True\n",
      "1538247876 1538247876 True\n",
      "1663839363 1663839363 True\n",
      "1487601689 1487601689 True\n",
      "accuracy is: 1\n"
     ]
    }
   ],
   "source": [
    "train(3000, model, optimizer)\n",
    "evaluate(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
